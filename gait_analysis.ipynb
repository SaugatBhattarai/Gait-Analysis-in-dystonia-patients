{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procrustes(X, Y, scaling=True, reflection='best'):\n",
    "    \"\"\"\n",
    "    A port of MATLAB's `procrustes` function to Numpy.\n",
    "\n",
    "    Procrustes analysis determines a linear transformation (translation,\n",
    "    reflection, orthogonal rotation and scaling) of the points in Y to best\n",
    "    conform them to the points in matrix X, using the sum of squared errors\n",
    "    as the goodness of fit criterion.\n",
    "\n",
    "        d, Z, [tform] = procrustes(X, Y)\n",
    "\n",
    "    Inputs:\n",
    "    ------------\n",
    "    X, Y    \n",
    "        matrices of target and input coordinates. they must have equal\n",
    "        numbers of  points (rows), but Y may have fewer dimensions\n",
    "        (columns) than X.\n",
    "\n",
    "    scaling \n",
    "        if False, the scaling component of the transformation is forced\n",
    "        to 1\n",
    "\n",
    "    reflection\n",
    "        if 'best' (default), the transformation solution may or may not\n",
    "        include a reflection component, depending on which fits the data\n",
    "        best. setting reflection to True or False forces a solution with\n",
    "        reflection or no reflection respectively.\n",
    "\n",
    "    Outputs\n",
    "    ------------\n",
    "    d       \n",
    "        the residual sum of squared errors, normalized according to a\n",
    "        measure of the scale of X, ((X - X.mean(0))**2).sum()\n",
    "\n",
    "    Z\n",
    "        the matrix of transformed Y-values\n",
    "\n",
    "    tform   \n",
    "        a dict specifying the rotation, translation and scaling that\n",
    "        maps X --> Y\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n,m = X.shape\n",
    "    ny,my = Y.shape\n",
    "\n",
    "    muX = X.mean(0)\n",
    "    muY = Y.mean(0)\n",
    "\n",
    "    X0 = X - muX\n",
    "    Y0 = Y - muY\n",
    "\n",
    "    ssX = (X0**2.).sum()\n",
    "    ssY = (Y0**2.).sum()\n",
    "\n",
    "    # centred Frobenius norm\n",
    "    normX = np.sqrt(ssX)\n",
    "    normY = np.sqrt(ssY)\n",
    "\n",
    "    # scale to equal (unit) norm\n",
    "    X0 /= normX\n",
    "    Y0 /= normY\n",
    "\n",
    "    if my < m:\n",
    "        Y0 = np.concatenate((Y0, np.zeros(n, m-my)),0)\n",
    "\n",
    "    # optimum rotation matrix of Y\n",
    "    A = np.dot(X0.T, Y0)\n",
    "    U,s,Vt = np.linalg.svd(A,full_matrices=False)\n",
    "    V = Vt.T\n",
    "    T = np.dot(V, U.T)\n",
    "\n",
    "    if reflection is not 'best':\n",
    "\n",
    "        # does the current solution use a reflection?\n",
    "        have_reflection = np.linalg.det(T) < 0\n",
    "\n",
    "        # if that's not what was specified, force another reflection\n",
    "        if reflection != have_reflection:\n",
    "            V[:,-1] *= -1\n",
    "            s[-1] *= -1\n",
    "            T = np.dot(V, U.T)\n",
    "\n",
    "    traceTA = s.sum()\n",
    "\n",
    "    if scaling:\n",
    "\n",
    "        # optimum scaling of Y\n",
    "        b = traceTA * normX / normY\n",
    "\n",
    "        # standarised distance between X and b*Y*T + c\n",
    "        d = 1 - traceTA**2\n",
    "\n",
    "        # transformed coords\n",
    "        Z = normX*traceTA*np.dot(Y0, T) + muX\n",
    "\n",
    "    else:\n",
    "        b = 1\n",
    "        d = 1 + ssY/ssX - 2 * traceTA * normY / normX\n",
    "        Z = normY*np.dot(Y0, T) + muX\n",
    "\n",
    "    # transformation matrix\n",
    "    if my < m:\n",
    "        T = T[:my,:]\n",
    "    c = muX - b*np.dot(muY, T)\n",
    "\n",
    "    #transformation values \n",
    "    tform = {'rotation':T, 'scale':b, 'translation':c}\n",
    "\n",
    "    return d, Z, tform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fb7c49cf588>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "torch.set_printoptions(linewidth=120) #Display options for output\n",
    "torch.set_grad_enabled(True) #Already on by default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1+cu101\n",
      "0.6.1+cu101\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare Data Train set and validationset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate number of correct prediction given labels\n",
    "def get_num_correct(preds,labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    normalized_data = pd.read_csv('dystonia_dataset_train_normalized.csv')\n",
    "    \n",
    "    X = normalized_data['image']\n",
    "    Y = normalized_data['class']\n",
    "\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=3)\n",
    "    \n",
    "    X_train_Compiled = np.empty((0,50,51,1))\n",
    "    X_test_Compiled = np.empty((0,50,51,1))\n",
    "    Y_test_compiled = []\n",
    "    Y_train_compiled = []\n",
    "    \n",
    "    for index in X_train.index:\n",
    "        json_load = json.loads(X_train[index])\n",
    "        a_restored = np.asarray(json_load)\n",
    "        X_train_Compiled=np.append(X_train_Compiled,[a_restored],axis=0)\n",
    "        X_train_Compiled=X_train_Compiled.astype(np.float32)\n",
    "        \n",
    "    for index in X_test.index:\n",
    "        json_load = json.loads(X_test[index])\n",
    "        a_restored = np.asarray(json_load)\n",
    "        X_test_Compiled=np.append(X_test_Compiled,[a_restored],axis=0)\n",
    "        X_test_Compiled=X_test_Compiled.astype(np.float32)\n",
    "        \n",
    "    for index in Y_test.index:\n",
    "        Y_test_compiled.append(Y_test[index])\n",
    "        \n",
    "    for index in Y_train.index:\n",
    "        Y_train_compiled.append(Y_train[index])\n",
    "        \n",
    "    return X_train_Compiled,Y_train_compiled,X_test_Compiled,Y_test_compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images,train_labels,test_images,test_labels = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train_images.mean()\n",
    "train_std = train_images.std()\n",
    "\n",
    "test_mean = test_images.mean()\n",
    "test_std = test_images.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.009724988, 0.27727515)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean,train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.008603053, 0.27681193)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mean,test_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Test Data (referred as validationset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data():\n",
    "    normalized_data = pd.read_csv('dystonia_dataset_test_normalized.csv')\n",
    "\n",
    "    X_test = normalized_data['image']\n",
    "    Y_test = normalized_data['class']\n",
    "\n",
    "    X_test_Compiled = np.empty((0,50,51,1))\n",
    "    Y_test_Compiled = []\n",
    "\n",
    "        \n",
    "    for index in X_test.index:\n",
    "        json_load = json.loads(X_test[index])\n",
    "        a_restored = np.asarray(json_load)\n",
    "        X_test_Compiled=np.append(X_test_Compiled,[a_restored],axis=0)\n",
    "        X_test_Compiled=X_test_Compiled.astype(np.float32)\n",
    "        \n",
    "    for index in Y_test.index:\n",
    "        Y_test_Compiled.append(Y_test[index])\n",
    "            \n",
    "    return X_test_Compiled,Y_test_Compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images, val_labels = preprocess_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mean = val_images.mean()\n",
    "val_std = val_images.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.006763445, 0.27508262)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_mean,val_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset\n",
    "class DystoniaDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images, labels=None, transforms=None):\n",
    "        self.X = images\n",
    "        self.y = labels\n",
    "        self.transforms = transforms\n",
    "        self.targets = []\n",
    "        self.classes = ['0 GDS','1 GDS','2 GDS']\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def train_labels(self):\n",
    "        self.targets = train_labels\n",
    "        return self.targets\n",
    "\n",
    "    @property\n",
    "    def test_labels(self): \n",
    "        self.targets = test_labels\n",
    "        return self.targets\n",
    "    \n",
    "    @property\n",
    "    def val_labels(self): \n",
    "        self.targets = val_labels\n",
    "        return self.targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        data = self.X[i]\n",
    "        if self.transforms:\n",
    "            data = self.transforms(data)\n",
    "            \n",
    "        if self.y is not None:\n",
    "            return (data, self.y[i])\n",
    "        else:\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(num_features=16)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=32)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=64)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=128)\n",
    "        self.bn5 = nn.BatchNorm1d(num_features=1000)\n",
    "            \n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=128 * 6 * 6, out_features=1000)\n",
    "        self.fc2 = nn.Linear(in_features=1000, out_features=100)\n",
    "        self.out = nn.Linear(in_features=100, out_features=3)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "#         print('INput layer', t)\n",
    "#         print('Input layer Shpae', t.shape)\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.bn1(t)\n",
    "        \n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        t = self.bn2(t)\n",
    "        \n",
    "        # dropout 0.3\n",
    "        t = self.dropout1(t)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv3(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.bn3(t)\n",
    "        \n",
    "        t = self.conv4(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        t = self.bn4(t)\n",
    "\n",
    "\n",
    "        # dropout 0.4\n",
    "        t = self.dropout2(t)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 128 * 6 * 6)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.bn5(t)\n",
    "\n",
    "        # dropout 0.5\n",
    "        t = self.dropout3(t)\n",
    "\n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms\n",
    "train_transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([train_mean],[train_std])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([test_mean],[test_std])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([val_mean],[val_std])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DystoniaDataset(train_images, train_labels, train_transform)\n",
    "test_set = DystoniaDataset(test_images, test_labels, test_transform)\n",
    "val_set = DystoniaDataset(val_images, val_labels, val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=1)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=True, num_workers=1)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn5): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=4608, out_features=1000, bias=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (out): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network = Network().to(device)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "# optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = optim.Adam(network.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_preds(model,loader):\n",
    "    all_preds = torch.tensor([]).to(device)\n",
    "    for batch in loader:\n",
    "        images = batch[0].to(device,non_blocking=True)\n",
    "        labels = batch[1].to(device,non_blocking=True)\n",
    "        \n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat(\n",
    "        (all_preds,preds),\n",
    "            dim=0\n",
    "        )\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network,train_loader):\n",
    "    \n",
    "    optimizer = optim.Adam(network.parameters(),lr=0.0001)\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            images = batch[0].to(device,non_blocking=True)\n",
    "            labels = batch[1].to(device,non_blocking=True)\n",
    "            \n",
    "            #Forward Pass\n",
    "            preds = network(images) #pass Batch to the network and get preds tensor\n",
    "            loss = F.cross_entropy(preds,labels) #calculate loss tensor\n",
    "#             loss = criterion(preds, labels)\n",
    "\n",
    "            #Backward Pass and optimize\n",
    "            #https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "            optimizer.zero_grad() #zero our gradients \n",
    "        \n",
    "            #Calculating Gradients \n",
    "            loss.backward() \n",
    "            \n",
    "            #Update Weights (step in the direction of loss funtions minimum:)\n",
    "            #optimizer updates the networks weight using gradients and learning rate\n",
    "            optimizer.step() \n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_correct += get_num_correct(preds,labels)\n",
    "        \n",
    "        print('epoch: ', epoch, 'total_correct', total_correct, 'loss', total_loss)\n",
    "    print('Done Training')\n",
    "    \n",
    "def test(network, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data[0].to(device, non_blocking=True), data[1].to(device, non_blocking=True)\n",
    "            outputs = network(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on test images: %0.3f %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_new(network,test_loader):\n",
    "    with torch.no_grad():\n",
    "        prediction_loader = torch.utils.data.DataLoader(test_set,batch_size=64)\n",
    "        test_preds = get_all_preds(network,prediction_loader)\n",
    "    targets = torch.tensor(test_set.test_labels).to(device)\n",
    "    preds_correct = get_num_correct(test_preds, targets)\n",
    "    print('Total Correct: ', preds_correct)\n",
    "    print('test accuracy', preds_correct / len(test_set))\n",
    "    \n",
    "def val_new(network,val_loader):\n",
    "    with torch.no_grad():\n",
    "        prediction_loader = torch.utils.data.DataLoader(val_set,batch_size=64)\n",
    "        val_preds = get_all_preds(network,prediction_loader)\n",
    "    targets = torch.tensor(val_set.val_labels).to(device)\n",
    "    preds_correct = get_num_correct(val_preds, targets)\n",
    "    print('Total Correct: ', preds_correct)\n",
    "    print('val accuracy', preds_correct / len(val_set)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 total_correct 479 loss 0.22821826115250587\n",
      "epoch:  1 total_correct 482 loss 0.08344553271308541\n",
      "epoch:  2 total_correct 482 loss 0.07037812017370015\n",
      "epoch:  3 total_correct 483 loss 0.05028401443269104\n",
      "epoch:  4 total_correct 484 loss 0.013520664244424552\n",
      "epoch:  5 total_correct 483 loss 0.05038601323030889\n",
      "epoch:  6 total_correct 482 loss 0.10596986627206206\n",
      "epoch:  7 total_correct 483 loss 0.06420086231082678\n",
      "epoch:  8 total_correct 483 loss 0.032667651772499084\n",
      "epoch:  9 total_correct 484 loss 0.01228137977886945\n",
      "epoch:  10 total_correct 483 loss 0.021238770335912704\n",
      "epoch:  11 total_correct 482 loss 0.06439856020733714\n",
      "epoch:  12 total_correct 482 loss 0.07950056972913444\n",
      "epoch:  13 total_correct 484 loss 0.013687115162611008\n",
      "epoch:  14 total_correct 484 loss 0.01967987697571516\n",
      "epoch:  15 total_correct 484 loss 0.0208441954528098\n",
      "epoch:  16 total_correct 482 loss 0.06327204452827573\n",
      "epoch:  17 total_correct 483 loss 0.026618073927238584\n",
      "epoch:  18 total_correct 484 loss 0.007022462785243988\n",
      "epoch:  19 total_correct 483 loss 0.042913319543004036\n",
      "epoch:  20 total_correct 483 loss 0.08924233293510042\n",
      "epoch:  21 total_correct 483 loss 0.07389169721864164\n",
      "epoch:  22 total_correct 484 loss 0.013539272476918995\n",
      "epoch:  23 total_correct 484 loss 0.03220814419910312\n",
      "epoch:  24 total_correct 484 loss 0.028915304224938154\n",
      "epoch:  25 total_correct 483 loss 0.06109095341525972\n",
      "epoch:  26 total_correct 483 loss 0.05404471233487129\n",
      "epoch:  27 total_correct 484 loss 0.038959172554314137\n",
      "epoch:  28 total_correct 484 loss 0.026274309027940035\n",
      "epoch:  29 total_correct 484 loss 0.01751642135786824\n",
      "epoch:  30 total_correct 483 loss 0.051024192944169044\n",
      "epoch:  31 total_correct 484 loss 0.005389650235883892\n",
      "epoch:  32 total_correct 483 loss 0.059836532920598984\n",
      "epoch:  33 total_correct 484 loss 0.014665429189335555\n",
      "epoch:  34 total_correct 484 loss 0.013779661967419088\n",
      "epoch:  35 total_correct 484 loss 0.02890982571989298\n",
      "epoch:  36 total_correct 483 loss 0.04620126872760011\n",
      "epoch:  37 total_correct 484 loss 0.031188313747406937\n",
      "epoch:  38 total_correct 483 loss 0.08605681359767914\n",
      "epoch:  39 total_correct 484 loss 0.025951535324566066\n",
      "epoch:  40 total_correct 483 loss 0.030393097549676895\n",
      "epoch:  41 total_correct 484 loss 0.03298100735992193\n",
      "epoch:  42 total_correct 483 loss 0.034600814804434776\n",
      "epoch:  43 total_correct 484 loss 0.020224583859089762\n",
      "epoch:  44 total_correct 483 loss 0.03787576826289296\n",
      "epoch:  45 total_correct 484 loss 0.03761361539363861\n",
      "epoch:  46 total_correct 484 loss 0.007740286644548178\n",
      "epoch:  47 total_correct 484 loss 0.010500734380912036\n",
      "epoch:  48 total_correct 483 loss 0.02920188382267952\n",
      "epoch:  49 total_correct 484 loss 0.015465298027265817\n",
      "epoch:  50 total_correct 484 loss 0.009157756139757112\n",
      "epoch:  51 total_correct 484 loss 0.0089188814163208\n",
      "epoch:  52 total_correct 484 loss 0.023103542625904083\n",
      "epoch:  53 total_correct 484 loss 0.010877580483793281\n",
      "epoch:  54 total_correct 483 loss 0.04357906011864543\n",
      "epoch:  55 total_correct 482 loss 0.042457871546503156\n",
      "epoch:  56 total_correct 484 loss 0.011341095087118447\n",
      "epoch:  57 total_correct 483 loss 0.041185807436704636\n",
      "epoch:  58 total_correct 484 loss 0.01669762036181055\n",
      "epoch:  59 total_correct 483 loss 0.07131592743098736\n",
      "epoch:  60 total_correct 483 loss 0.06356396013870835\n",
      "epoch:  61 total_correct 481 loss 0.10224957950413227\n",
      "epoch:  62 total_correct 484 loss 0.020992505364120007\n",
      "epoch:  63 total_correct 484 loss 0.04145472520031035\n",
      "epoch:  64 total_correct 484 loss 0.0315440664999187\n",
      "epoch:  65 total_correct 483 loss 0.04064704477787018\n",
      "epoch:  66 total_correct 484 loss 0.029231639578938484\n",
      "epoch:  67 total_correct 484 loss 0.005716392770409584\n",
      "epoch:  68 total_correct 484 loss 0.010788760664581787\n",
      "epoch:  69 total_correct 483 loss 0.03823077782180917\n",
      "epoch:  70 total_correct 484 loss 0.012546124868094921\n",
      "epoch:  71 total_correct 484 loss 0.0041823739011306316\n",
      "epoch:  72 total_correct 484 loss 0.0111943653319031\n",
      "epoch:  73 total_correct 482 loss 0.052116320468485355\n",
      "epoch:  74 total_correct 483 loss 0.03426887793466449\n",
      "epoch:  75 total_correct 483 loss 0.036583689448889345\n",
      "epoch:  76 total_correct 484 loss 0.01588832793640904\n",
      "epoch:  77 total_correct 483 loss 0.12840119749307632\n",
      "epoch:  78 total_correct 484 loss 0.011766308918595314\n",
      "epoch:  79 total_correct 484 loss 0.03824166243430227\n",
      "epoch:  80 total_correct 483 loss 0.05401461943984032\n",
      "epoch:  81 total_correct 484 loss 0.009127645651460625\n",
      "epoch:  82 total_correct 484 loss 0.01504746824502945\n",
      "epoch:  83 total_correct 481 loss 0.06879435293376446\n",
      "epoch:  84 total_correct 484 loss 0.009706918150186539\n",
      "epoch:  85 total_correct 483 loss 0.03822702224715613\n",
      "epoch:  86 total_correct 482 loss 0.0712107788422145\n",
      "epoch:  87 total_correct 484 loss 0.008818924659863114\n",
      "epoch:  88 total_correct 484 loss 0.017570150404935703\n",
      "epoch:  89 total_correct 483 loss 0.02587730309460312\n",
      "epoch:  90 total_correct 483 loss 0.026790586300194263\n",
      "epoch:  91 total_correct 484 loss 0.016913078725337982\n",
      "epoch:  92 total_correct 483 loss 0.018668743679882027\n",
      "epoch:  93 total_correct 483 loss 0.023839395493268967\n",
      "epoch:  94 total_correct 484 loss 0.0073030064813792706\n",
      "epoch:  95 total_correct 484 loss 0.011941179633140564\n",
      "epoch:  96 total_correct 484 loss 0.016273991903290153\n",
      "epoch:  97 total_correct 484 loss 0.011721450835466385\n",
      "epoch:  98 total_correct 484 loss 0.00628255121409893\n",
      "epoch:  99 total_correct 484 loss 0.01426179870031774\n",
      "epoch:  100 total_correct 484 loss 0.011165987234562635\n",
      "epoch:  101 total_correct 484 loss 0.007991008693352342\n",
      "epoch:  102 total_correct 483 loss 0.02375546982511878\n",
      "epoch:  103 total_correct 484 loss 0.029997711069881916\n",
      "epoch:  104 total_correct 484 loss 0.005347722937585786\n",
      "epoch:  105 total_correct 484 loss 0.01309241191484034\n",
      "epoch:  106 total_correct 484 loss 0.00865062657976523\n",
      "epoch:  107 total_correct 484 loss 0.007387625752016902\n",
      "epoch:  108 total_correct 484 loss 0.012242219119798392\n",
      "epoch:  109 total_correct 483 loss 0.030299620470032096\n",
      "epoch:  110 total_correct 484 loss 0.014980632811784744\n",
      "epoch:  111 total_correct 484 loss 0.019906754314433783\n",
      "epoch:  112 total_correct 483 loss 0.04182367480825633\n",
      "epoch:  113 total_correct 482 loss 0.07192625850439072\n",
      "epoch:  114 total_correct 484 loss 0.007672225008718669\n",
      "epoch:  115 total_correct 483 loss 0.045234109791636\n",
      "epoch:  116 total_correct 483 loss 0.04859280420350842\n",
      "epoch:  117 total_correct 484 loss 0.012841793242841959\n",
      "epoch:  118 total_correct 483 loss 0.041540074060321786\n",
      "epoch:  119 total_correct 484 loss 0.0041908398270606995\n",
      "epoch:  120 total_correct 481 loss 0.1024963017553091\n",
      "epoch:  121 total_correct 483 loss 0.01892385073006153\n",
      "epoch:  122 total_correct 483 loss 0.025422432809136808\n",
      "epoch:  123 total_correct 484 loss 0.02234025951474905\n",
      "epoch:  124 total_correct 483 loss 0.09457493480294943\n",
      "epoch:  125 total_correct 483 loss 0.03627447783946991\n",
      "epoch:  126 total_correct 483 loss 0.10277064144611359\n",
      "epoch:  127 total_correct 484 loss 0.010146301239728928\n",
      "epoch:  128 total_correct 484 loss 0.00926072191214189\n",
      "epoch:  129 total_correct 484 loss 0.014292672123701777\n",
      "epoch:  130 total_correct 484 loss 0.011791479890234768\n",
      "epoch:  131 total_correct 483 loss 0.04572881758213043\n",
      "epoch:  132 total_correct 482 loss 0.0489652682445012\n",
      "epoch:  133 total_correct 483 loss 0.04433575877919793\n",
      "epoch:  134 total_correct 484 loss 0.006408296525478363\n",
      "epoch:  135 total_correct 483 loss 0.035091233759885654\n",
      "epoch:  136 total_correct 484 loss 0.004794535576365888\n",
      "epoch:  137 total_correct 483 loss 0.039903152734041214\n",
      "epoch:  138 total_correct 484 loss 0.006863542308565229\n",
      "epoch:  139 total_correct 484 loss 0.018268281695782207\n",
      "epoch:  140 total_correct 484 loss 0.014407588372705504\n",
      "epoch:  141 total_correct 483 loss 0.030460838635917753\n",
      "epoch:  142 total_correct 484 loss 0.0072927423170767725\n",
      "epoch:  143 total_correct 484 loss 0.0032063350081443787\n",
      "epoch:  144 total_correct 484 loss 0.009932126849889755\n",
      "epoch:  145 total_correct 484 loss 0.008857927386998199\n",
      "epoch:  146 total_correct 483 loss 0.07916886420571245\n",
      "epoch:  147 total_correct 482 loss 0.09567757864715531\n",
      "epoch:  148 total_correct 483 loss 0.022961273789405823\n",
      "epoch:  149 total_correct 484 loss 0.025004906812682748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  150 total_correct 483 loss 0.06090780906379223\n",
      "epoch:  151 total_correct 484 loss 0.005054345980170183\n",
      "epoch:  152 total_correct 484 loss 0.02005778905004263\n",
      "epoch:  153 total_correct 484 loss 0.006210735999047756\n",
      "epoch:  154 total_correct 484 loss 0.016921856336921337\n",
      "epoch:  155 total_correct 484 loss 0.016542718978598714\n",
      "epoch:  156 total_correct 482 loss 0.0680966661311686\n",
      "epoch:  157 total_correct 484 loss 0.005243563326075673\n",
      "epoch:  158 total_correct 484 loss 0.011887714674230665\n",
      "epoch:  159 total_correct 484 loss 0.004370710710645653\n",
      "epoch:  160 total_correct 483 loss 0.03293235274031758\n",
      "epoch:  161 total_correct 484 loss 0.0039056282839737833\n",
      "epoch:  162 total_correct 484 loss 0.016846611397340894\n",
      "epoch:  163 total_correct 484 loss 0.014256746391765773\n",
      "epoch:  164 total_correct 484 loss 0.007072289357893169\n",
      "epoch:  165 total_correct 484 loss 0.005943969823420048\n",
      "epoch:  166 total_correct 483 loss 0.06987468525767326\n",
      "epoch:  167 total_correct 483 loss 0.038715314120054245\n",
      "epoch:  168 total_correct 482 loss 0.04555293524754234\n",
      "epoch:  169 total_correct 484 loss 0.0109026150603313\n",
      "epoch:  170 total_correct 483 loss 0.02439064218197018\n",
      "epoch:  171 total_correct 484 loss 0.015058995224535465\n",
      "epoch:  172 total_correct 483 loss 0.04610022623091936\n",
      "epoch:  173 total_correct 484 loss 0.020129965851083398\n",
      "epoch:  174 total_correct 484 loss 0.01772894896566868\n",
      "epoch:  175 total_correct 483 loss 0.0774733180005569\n",
      "epoch:  176 total_correct 483 loss 0.024361276533454657\n",
      "epoch:  177 total_correct 483 loss 0.20315680280327797\n",
      "epoch:  178 total_correct 482 loss 0.13734953975654207\n",
      "epoch:  179 total_correct 484 loss 0.012324572540819645\n",
      "epoch:  180 total_correct 483 loss 0.028911840377986664\n",
      "epoch:  181 total_correct 483 loss 0.06981229409575462\n",
      "epoch:  182 total_correct 484 loss 0.027647310867905617\n",
      "epoch:  183 total_correct 483 loss 0.03716884180903435\n",
      "epoch:  184 total_correct 484 loss 0.006050034120562486\n",
      "epoch:  185 total_correct 484 loss 0.0061351106414804235\n",
      "epoch:  186 total_correct 484 loss 0.015022092964500189\n",
      "epoch:  187 total_correct 483 loss 0.029030784033238888\n",
      "epoch:  188 total_correct 483 loss 0.03834071278106421\n",
      "epoch:  189 total_correct 483 loss 0.014661734719993547\n",
      "epoch:  190 total_correct 484 loss 0.003538188830134459\n",
      "epoch:  191 total_correct 484 loss 0.02227067667990923\n",
      "epoch:  192 total_correct 484 loss 0.004896582002402283\n",
      "epoch:  193 total_correct 483 loss 0.018797984113916755\n",
      "epoch:  194 total_correct 483 loss 0.01548056717729196\n",
      "epoch:  195 total_correct 484 loss 0.013296948425704613\n",
      "epoch:  196 total_correct 484 loss 0.0015055884905450512\n",
      "epoch:  197 total_correct 484 loss 0.002129238724592142\n",
      "epoch:  198 total_correct 484 loss 0.004622510343324393\n",
      "epoch:  199 total_correct 484 loss 0.01751027794671245\n",
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "train(network, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test(network, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_new(network, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_new(network,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Correct:  484\n",
      "Total Train Set 484\n",
      "accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_loader = torch.utils.data.DataLoader(train_set,batch_size=64)\n",
    "    train_preds = get_all_preds(network,prediction_loader)\n",
    "targets = torch.tensor(train_set.train_labels).to(device)\n",
    "preds_correct = get_num_correct(train_preds, targets)\n",
    "print('Total Correct: ', preds_correct)\n",
    "print('Total Train Set', len(train_set))\n",
    "print('accuracy', preds_correct / len(train_set)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Correct:  104\n",
      "Total test set 121\n",
      "accuracy 0.859504132231405\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_loader = torch.utils.data.DataLoader(test_set,batch_size=64)\n",
    "    test_preds = get_all_preds(network,prediction_loader)\n",
    "targets = torch.tensor(test_set.test_labels).to(device)\n",
    "preds_correct = get_num_correct(test_preds, targets)\n",
    "print('Total Correct: ', preds_correct)\n",
    "print('Total test set', len(test_set))\n",
    "print('accuracy', preds_correct / len(test_set)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Correct:  24\n",
      "Total val set 60\n",
      "val accuracy 0.4\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_loader = torch.utils.data.DataLoader(val_set,batch_size=64)\n",
    "    val_preds = get_all_preds(network,prediction_loader)\n",
    "targets = torch.tensor(val_set.val_labels).to(device)\n",
    "preds_correct = get_num_correct(val_preds, targets)\n",
    "print('Total Correct: ', preds_correct)\n",
    "print('Total val set', len(val_set))\n",
    "print('val accuracy', preds_correct / len(val_set)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Confusion Matrix for Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Confusion Matrix\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]], dtype=torch.int32)\n",
      "\n",
      "\n",
      "Confusion Matrix for Train Set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[157,   0,   0],\n",
       "        [  0, 157,   0],\n",
       "        [  0,   0, 170]], dtype=torch.int32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_set.targets\n",
    "targets = torch.tensor(train_set.train_labels).to(device)\n",
    "\n",
    "stacked = torch.stack(\n",
    "    (\n",
    "        targets,\n",
    "        train_preds.argmax(dim=1)\n",
    "    ),\n",
    "    dim=1\n",
    ")\n",
    "cmt = torch.zeros(3,3,dtype=torch.int32)\n",
    "\n",
    "print('Initialized Confusion Matrix')\n",
    "print(cmt)\n",
    "print('\\n')\n",
    "for p in stacked:\n",
    "    tl, pl = p.tolist()\n",
    "    cmt[tl,pl] = cmt[tl,pl] + 1\n",
    "print('Confusion Matrix for Train Set')\n",
    "cmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Confusion Matrix for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Validation set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[45,  1,  0],\n",
       "        [ 3, 33,  7],\n",
       "        [ 2,  4, 26]], dtype=torch.int32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = torch.tensor(test_set.test_labels).to(device)\n",
    "\n",
    "stacked = torch.stack(\n",
    "    (\n",
    "        targets,\n",
    "        test_preds.argmax(dim=1)\n",
    "    ),\n",
    "    dim=1\n",
    ")\n",
    "cmt = torch.zeros(3,3,dtype=torch.int32)\n",
    "for p in stacked:\n",
    "    j,k = p.tolist()\n",
    "    cmt[j,k] = cmt[j,k] + 1\n",
    "print('Confusion Matrix for Validation set')\n",
    "cmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building COnfusion Matrix for Test Set (here named as val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Validation set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 8, 10,  2],\n",
       "        [ 5,  2, 13],\n",
       "        [ 0,  6, 14]], dtype=torch.int32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = torch.tensor(val_set.val_labels).to(device)\n",
    "\n",
    "stacked = torch.stack(\n",
    "    (\n",
    "        targets,\n",
    "        val_preds.argmax(dim=1)\n",
    "    ),\n",
    "    dim=1\n",
    ")\n",
    "cmt = torch.zeros(3,3,dtype=torch.int32)\n",
    "for p in stacked:\n",
    "    j,k = p.tolist()\n",
    "    cmt[j,k] = cmt[j,k] + 1\n",
    "print('Confusion Matrix for Validation set')\n",
    "cmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, Recall and F1 Score of indiviuals of validation set (here test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three labels 0,1,2 for gps of 0, 1 , 5 respectively. Change value gds level 0 to any other value for calculating individuals\n",
    "Modified from: # https://stackoverflow.com/questions/56643503/efficient-metrics-evaluation-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_true = 0\n",
    "predicted_true=0\n",
    "correct_true=0\n",
    "gds_label = 2 #0, 1, 2 class\n",
    "\n",
    "targets = torch.tensor(test_set.test_labels).to(device)\n",
    "predicted_classes = torch.argmax(test_preds, dim=1) == gds_label\n",
    "target_classes = targets == gds_label\n",
    "\n",
    "\n",
    "target_true += torch.sum(targets == gds_label).float()\n",
    "predicted_true += torch.sum(predicted_classes).float()\n",
    "correct_true += torch.sum(target_classes * predicted_classes == True)\n",
    "# print(correct_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = correct_true / target_true\n",
    "precision = correct_true / predicted_true\n",
    "f1_score = 2 * (precision * recall)/ (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8125, device='cuda:0')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8182, device='cuda:0')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8308, device='cuda:0')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(47., device='cuda:0')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43, device='cuda:0')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(46., device='cuda:0')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision, Recall and F1 Score of Test set here referred as val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_true = 0\n",
    "predicted_true=0\n",
    "correct_true=0\n",
    "gds_label = 2 #0, 1, 2 class\n",
    "\n",
    "targets = torch.tensor(val_set.val_labels).to(device)\n",
    "predicted_classes = torch.argmax(val_preds, dim=1) == gds_label\n",
    "target_classes = targets == gds_label\n",
    "\n",
    "\n",
    "target_true += torch.sum(targets == gds_label).float()\n",
    "predicted_true += torch.sum(predicted_classes).float()\n",
    "correct_true += torch.sum(target_classes * predicted_classes == True)\n",
    "# print(correct_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = correct_true / target_true\n",
    "precision = correct_true / predicted_true\n",
    "f1_score = 2 * (precision * recall)/ (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4500, device='cuda:0')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4286, device='cuda:0')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4390, device='cuda:0')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20., device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
